{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "import bs4 as bs\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "#pretty print package\n",
    "from pprint import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "service = Service(executable_path=\"/Users/shivani/anaconda3/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.glassdoor.com/Job/germany-data-scientist-jobs-SRCH_IL.0,7_IN96_KO8,22.htm?clickSource=searchBox'\n",
    "gs_html = driver.get(url)\n",
    "time.sleep(4)\n",
    "pageSource = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for the \"Sign Up\" prompt and get rid of it.\n",
    "try:\n",
    "    driver.find_element(by=By.CLASS_NAME,value=\"selected\").click()\n",
    "except ElementClickInterceptedException:\n",
    "    pass\n",
    "\n",
    "time.sleep(.1)\n",
    "\n",
    "try:\n",
    "    driver.find_element(by=By.XPATH, value='/html/body/div[12]/div/div/div[2]/span').click()  #clicking to the X.\n",
    "except NoSuchElementException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To accept cookies\n",
    "try:\n",
    "    driver.find_element(by=By.ID,value='onetrust-accept-btn-handler').click()\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving source page into html \n",
    "fileToWrite = open(\"page_source.html\", \"w\")\n",
    "fileToWrite.write(pageSource)\n",
    "fileToWrite.close()\n",
    "fileToRead = open(\"page_source.html\", \"r\")\n",
    "pprint(fileToRead.read())\n",
    "fileToRead.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"page_source.html\", \"r\") as file:\n",
    "    gs_jobs= file.read()\n",
    "gs_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create soup object\n",
    "soup = bs.BeautifulSoup(gs_jobs, \"html.parser\")\n",
    "#base_url = \"https://www.glassdoor.com/\"\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_from_html(gs_jobs):\n",
    "    \"\"\"\n",
    "    Extract Glassdoor job posting URLs from HTML\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create soup object\n",
    "    soup = bs.BeautifulSoup(gs_jobs, \"html.parser\")\n",
    "    base_url = \"https://www.glassdoor.com\"\n",
    "\n",
    "    ul = soup.find(\"ul\", {\"data-test\": \"jlGrid\"})\n",
    "\n",
    "    expose_urls = []\n",
    "    children = ul.find_all(\"li\", {\"class\": \"react-job-listing\"})\n",
    "\n",
    "    for child in children:\n",
    "        for link in child.find_all(\"a\"):\n",
    "            expose_urls.append(base_url + link['href'])\n",
    "    \n",
    "    return expose_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expose_urls = get_urls_from_html(gs_jobs)\n",
    "expose_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scrape the data from each link\n",
    "\n",
    "job_links=[]\n",
    "for index,url in enumerate(expose_urls):\n",
    "    pprint(url)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "    response = driver.page_source\n",
    "    job_links.append(response)\n",
    "    #pprint(response)\n",
    "    \n",
    "    with open(f\"./data/scraped_links/job{index}.htm\", \"w\") as f:\n",
    "       f.write(response)\n",
    "    \n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting details from the links\n",
    "\n",
    "def parse_html(html):\n",
    "    \n",
    "    soup = bs.BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    title = soup.find('div', {'data-test':'job-title'}).get_text()\n",
    "\n",
    "    location = soup.find(\"span\", {'data-test':'location'}).get_text()\n",
    "\n",
    "    company_name = soup.find(\"div\", class_=\"d-flex align-items-center justify-content-between css-16nw49e e11nt52q1\").get_text()\n",
    "\n",
    "    job_description = soup.find(\"div\", id=\"JobDescriptionContainer\").get_text()\n",
    "    \n",
    "    jobs= {\n",
    "        \"title\": title,\n",
    "        \"location\": location,\n",
    "        \"company_name\": company_name,\n",
    "        \"job_description\": job_description\n",
    "    }\n",
    "\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the path for links and create a data frame\n",
    "dir_html = Path('./data/scraped_links/')\n",
    "df = pd.DataFrame(columns=[\"title\", \"location\", \"company_name\", \"job_description\"])\n",
    "\n",
    "for file in dir_html.iterdir():\n",
    "    with open(file, \"r\") as f:\n",
    "       html = f.read()\n",
    "    \n",
    "    data = parse_html(html)\n",
    "    df.loc[len(df)] = data.values()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the data frame into csv file\n",
    "df.to_csv('./data/glassdoor_jobs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume_optimizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
